{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "print(common_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 拼接数据\n",
    "documents = [TaggedDocument(doc, [i]) for i,doc in enumerate(common_texts)]\n",
    "\n",
    "\"\"\"\n",
    "gensim.models.doc2vec.Doc2Vec(\n",
    "  documents=None, 文档集合, list(TaggedDocument(list(string), tagid))\n",
    "  corpus_file=None, \n",
    "  dm_mean=None, 如果使用PVDM结构的时候，对于上下文的单词向量是相加(0)还是均值(1)\n",
    "  dm=1, 训练算法；1表示PVDM，0表示PV-DBOW\n",
    "  dbow_words=0, 如果使用PVDMOW结构的时候，同时训练文档向量和词向量(1)，仅训练文档向量(0)\n",
    "  dm_concat=0, 如果使用PVDM结构的时候，单词向量和文档向量的合并方式为sum(0)还是concat(1)\n",
    "  dm_tag_count=1, \n",
    "  docvecs=None, \n",
    "  docvecs_mapfile=None, \n",
    "  comment=None, \n",
    "  trim_rule=None, \n",
    "  callbacks=(), **kwargs)\n",
    "NOTE: Word2Vec的所有参数Doc2Vec基本都支持\n",
    "\n",
    "\"\"\"\n",
    "# 模型训练\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【Doc2Vec结果】:\n",
      "[-0.04091445  0.04344336 -0.08487181  0.07065757 -0.03429239]\n"
     ]
    }
   ],
   "source": [
    "# 预测使用\n",
    "vector = model.infer_vector([\"system\", \"response\"])\n",
    "print(\"【Doc2Vec结果】:\\n{}\".format(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【Doc2Vec结果】:\n",
      "[-0.08389413 -0.06861168 -0.01032796  0.05317792  0.09081002]\n"
     ]
    }
   ],
   "source": [
    "# 预测使用\n",
    "vector = model.infer_vector([\"interface\", \"minors\", \"eps\", \"小明\"])\n",
    "print(\"【Doc2Vec结果】:\\n{}\".format(vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 模型对象创建\n",
    "\"\"\"\n",
    "gensim.models.fasttext.FastText(\n",
    "    sentences=None,  文档， list(list(string))\n",
    "    corpus_file=None, \n",
    "    sg=0,  和Word2Vec一样\n",
    "    hs=0,  和Word2Vec一样\n",
    "    size=100,  和Word2Vec一样\n",
    "    alpha=0.025, 和Word2Vec一样\n",
    "    window=5, 和Word2Vec一样\n",
    "    min_count=5, 和Word2Vec一样\n",
    "    max_vocab_size=None, 和Word2Vec一样\n",
    "    word_ngrams=1, 提取n-gram的时候，N等于多少\n",
    "    sample=0.001, \n",
    "    seed=1, \n",
    "    workers=3, \n",
    "    min_alpha=0.0001, \n",
    "    negative=5, \n",
    "    ns_exponent=0.75, \n",
    "    cbow_mean=1, hashfxn=<built-in function hash>, iter=5, \n",
    "    null_word=0, \n",
    "    min_n=3, \n",
    "    max_n=6, \n",
    "    sorted_vocab=1, \n",
    "    bucket=2000000, \n",
    "    trim_rule=None, \n",
    "    batch_words=10000, callbacks=(), compatible_hash=True)\n",
    "\n",
    "\"\"\"\n",
    "model = FastText(size=4, window=3, min_count=1)\n",
    "# 构建词表\n",
    "model.build_vocab(sentences=common_texts)\n",
    "# 训练模型\n",
    "model.train(sentences=common_texts, total_examples=model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03050808 -0.00921003  0.0233555   0.00851412]\n",
      "[ 0.0185172   0.03048961 -0.00839526  0.02434803]\n"
     ]
    }
   ],
   "source": [
    "# 模型应用（获取单词向量）\n",
    "v1 = model.wv.get_vector('system')\n",
    "v2 = model.wv.get_vector('interface')\n",
    "print(v1)\n",
    "print(v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、扩展\n",
    "直接读取文件训练FastText模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-11 15:04:12,416 : INFO : resetting layer weights\n",
      "2020-03-11 15:04:12,637 : INFO : collecting all words and their counts\n",
      "2020-03-11 15:04:12,639 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-11 15:04:12,777 : INFO : collected 17800 word types from a corpus of 131991 raw words and 2422 sentences\n",
      "2020-03-11 15:04:12,778 : INFO : Loading a fresh vocabulary\n",
      "2020-03-11 15:04:12,807 : INFO : effective_min_count=1 retains 17800 unique words (100% of original 17800, drops 0)\n",
      "2020-03-11 15:04:12,808 : INFO : effective_min_count=1 leaves 131991 word corpus (100% of original 131991, drops 0)\n",
      "2020-03-11 15:04:12,866 : INFO : deleting the raw counts dictionary of 17800 items\n",
      "2020-03-11 15:04:12,868 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2020-03-11 15:04:12,869 : INFO : downsampling leaves estimated 111227 word corpus (84.3% of prior 131991)\n",
      "2020-03-11 15:04:12,976 : INFO : estimated required memory for 17800 words, 67801 buckets and 4 dimensions: 11999032 bytes\n",
      "2020-03-11 15:04:12,978 : INFO : resetting layer weights\n",
      "2020-03-11 15:04:13,371 : INFO : training model with 3 workers on 17800 vocabulary and 4 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
      "2020-03-11 15:04:13,595 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 15:04:13,601 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 15:04:13,606 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 15:04:13,608 : INFO : EPOCH - 1 : training on 131991 raw words (111306 effective words) took 0.2s, 477636 effective words/s\n",
      "2020-03-11 15:04:13,811 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 15:04:13,813 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 15:04:13,820 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 15:04:13,822 : INFO : EPOCH - 2 : training on 131991 raw words (111219 effective words) took 0.2s, 533043 effective words/s\n",
      "2020-03-11 15:04:14,016 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 15:04:14,021 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 15:04:14,029 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 15:04:14,030 : INFO : EPOCH - 3 : training on 131991 raw words (111153 effective words) took 0.2s, 542594 effective words/s\n",
      "2020-03-11 15:04:14,230 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 15:04:14,233 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 15:04:14,240 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 15:04:14,241 : INFO : EPOCH - 4 : training on 131991 raw words (111258 effective words) took 0.2s, 536314 effective words/s\n",
      "2020-03-11 15:04:14,455 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 15:04:14,462 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 15:04:14,469 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 15:04:14,470 : INFO : EPOCH - 5 : training on 131991 raw words (111160 effective words) took 0.2s, 490056 effective words/s\n",
      "2020-03-11 15:04:14,667 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 15:04:14,671 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 15:04:14,678 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 15:04:14,679 : INFO : EPOCH - 6 : training on 131991 raw words (111309 effective words) took 0.2s, 540534 effective words/s\n",
      "2020-03-11 15:04:14,881 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 15:04:14,883 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 15:04:14,890 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 15:04:14,891 : INFO : EPOCH - 7 : training on 131991 raw words (111193 effective words) took 0.2s, 531944 effective words/s\n",
      "2020-03-11 15:04:15,081 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 15:04:15,086 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 15:04:15,088 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 15:04:15,089 : INFO : EPOCH - 8 : training on 131991 raw words (111251 effective words) took 0.2s, 569591 effective words/s\n",
      "2020-03-11 15:04:15,279 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 15:04:15,280 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 15:04:15,286 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 15:04:15,287 : INFO : EPOCH - 9 : training on 131991 raw words (111222 effective words) took 0.2s, 569944 effective words/s\n",
      "2020-03-11 15:04:15,482 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 15:04:15,484 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 15:04:15,489 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 15:04:15,490 : INFO : EPOCH - 10 : training on 131991 raw words (111123 effective words) took 0.2s, 553357 effective words/s\n",
      "2020-03-11 15:04:15,492 : INFO : training on a 1319910 raw words (1112194 effective words) took 2.1s, 524739 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim import utils\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "word_file_path = './datas/cut_words_of_in_the_name_of_people.txt'\n",
    "class MyData(object):\n",
    "    def __iter__(self):\n",
    "        path = word_file_path\n",
    "        with open(path, 'r', encoding='utf-8') as reader:\n",
    "            for line in reader:\n",
    "                yield list(utils.tokenize(line))\n",
    "\n",
    "# 模型构建\n",
    "model = FastText(size=4, window=3, min_count=1, sentences=MyData(), iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-11 15:04:15,768 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-03-11 15:04:15,770 : INFO : precomputing L2-norms of ngram weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "凑合着 0.9985167980194092\n",
      "高育良 0.9978237748146057\n",
      "里斯本 0.9968358874320984\n",
      "四五十 0.9964548945426941\n",
      "请育良 0.9963105320930481\n"
     ]
    }
   ],
   "source": [
    "# 夹角余弦相似度\n",
    "req_count = 5\n",
    "for key in model.wv.similar_by_word('沙瑞金', topn =100):\n",
    "    if len(key[0])==3:\n",
    "        req_count -= 1\n",
    "        print(key[0], key[1])\n",
    "        if req_count == 0:\n",
    "            break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
