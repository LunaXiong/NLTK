{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE\n",
    "在企业中使用Word2Vec一般的方式如下：<br/>\n",
    "1. 使用gensim这类的第三方框架在原始数据上训练一个Embedding Table(单词和词向量之间的映射表)<br/>\n",
    "2. 在业务中，需要使用到词向量转换的时候，直接加载这个Embedding Table作为单词向量转换的初始参数<br/>\n",
    "    在深度学习中我们一般使用tf.nn.embedding_lookup(embedding_table,word_idxs)来获取单词id对应的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import modules & set up logging\n",
    "import logging # 打印日志的模块，日志主要包括以下几个级别: DEBUG、INFO、WARN、ERROR\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import jieba.analyse\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 原始文本数据所在的磁盘路径\n",
    "sentence_file_path = './datas/in_the_name_of_people.txt'\n",
    "# 进行文本分词之后的数据存储磁盘路径\n",
    "word_file_path = './datas/cut_words_of_in_the_name_of_people.txt'\n",
    "# Word2Vec模型持久化保存的磁盘路径\n",
    "model_file_path1 = './datas/gensim_word2vec1.w2v'\n",
    "model_file_path2 = './datas/gensim_word2vec2.bin'\n",
    "model_file_path3 = './datas/gensim_word2vec3_{}.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对文件【./datas/in_the_name_of_people.txt】里面的内容进行分词!\n",
      "并且将分词结果数据保存到文件【./datas/cut_words_of_in_the_name_of_people.txt】中!\n",
      "Done!!!\n"
     ]
    }
   ],
   "source": [
    "# 人民的名义 小说分词\n",
    "jieba.suggest_freq('沙瑞金',True)\n",
    "jieba.suggest_freq('侯处长',True)\n",
    "jieba.suggest_freq('田国富',True)\n",
    "jieba.suggest_freq('高育良',True)\n",
    "jieba.suggest_freq('侯亮平',True)\n",
    "jieba.suggest_freq('钟小艾', True)\n",
    "jieba.suggest_freq('陈岩石', True)\n",
    "jieba.suggest_freq('欧阳菁', True)\n",
    "jieba.suggest_freq('易学习', True)\n",
    "jieba.suggest_freq('王大路', True)\n",
    "jieba.suggest_freq('蔡成功', True)\n",
    "jieba.suggest_freq('孙连城', True)\n",
    "jieba.suggest_freq('季昌明', True)\n",
    "jieba.suggest_freq('丁义珍', True)\n",
    "jieba.suggest_freq('郑西坡', True)\n",
    "jieba.suggest_freq('赵东来', True)\n",
    "jieba.suggest_freq('高小琴', True)\n",
    "jieba.suggest_freq('赵瑞龙', True)\n",
    "jieba.suggest_freq('林华华', True)\n",
    "jieba.suggest_freq('陆亦可', True)\n",
    "jieba.suggest_freq('刘新建', True)\n",
    "jieba.suggest_freq('刘庆祝', True)\n",
    "jieba.suggest_freq('京州市', True)\n",
    "jieba.suggest_freq('副市长', True)\n",
    "jieba.suggest_freq('赵德汉', True)\n",
    "jieba.suggest_freq('吴彩霞', True)\n",
    "jieba.add_word('陈海', 100)\n",
    "\n",
    "# 自定义词典\n",
    "jieba.add_word('人民的名义')\n",
    "jieba.add_word('数字版')\n",
    "jieba.add_word('中文在线数字出版集团股份有限公司')\n",
    "jieba.add_word('离婚法')\n",
    "\n",
    "print(\"对文件【{}】里面的内容进行分词!\".format(sentence_file_path))\n",
    "print(\"并且将分词结果数据保存到文件【{}】中!\".format(word_file_path))\n",
    "with open(word_file_path,'w', encoding='utf-8') as writer:\n",
    "    with open(sentence_file_path, 'r', encoding='utf-8') as reader:\n",
    "        # 加载所有数据\n",
    "        content = reader.read()\n",
    "        \n",
    "        # 分词\n",
    "        content = jieba.cut(content)\n",
    "        \n",
    "        # 合并结果\n",
    "        result = ' '.join(content)\n",
    "        \n",
    "        # 结果输出\n",
    "        writer.write(result)\n",
    "print(\"Done!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、Gensim Word2Vec构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "['五十三']\n",
      "==================================================\n",
      "['五十四']\n",
      "==================================================\n",
      "['一']\n",
      "==================================================\n",
      "['侯亮平', '得知', '航班', '无限期', '延误', '，', '急得', '差点', '跳', '起来', '。', '他本', '打算', '坐', '最后', '一班', '飞机', '赶往', 'H', '省', '，', '协调', '指挥', '抓捕', '京州市', '副市长', '丁义珍', '的', '行动', '，', '这下子', '计划', '全', '落空', '了', '。', '广播', '中', '一遍', '遍', '传来', '女', '播音员', '中英文', '抱歉', '的', '通知', '，', '机场', '上空', '有', '雷暴', '区', '，', '为了', '乘客', '安全', '，', '飞机', '暂时', '无法', '起飞', '。', '侯亮平', '额上', '沁出', '一层', '细细的', '汗珠', '，', '早', '知道', '被困', '机场', '的', '痛苦', '，', '现在', '又', '得', '尝', '一次', '滋味', '了', '。']\n",
      "==================================================\n",
      "['电视', '大', '荧屏', '正', '放映', '气象图', '，', '一', '团团', '浓厚', '的', '白云', '呈', '旋涡', '状', '翻卷', '，', '十分', '凶险', '的', '样子', '。', '字幕', '普及', '着', '航空', '知识', '—', '—', '雷暴', '如何', '危及', '飞行', '安全', '，', '误入', '雷暴', '区', '曾', '如何', '导致', '空难', '。', '但', '这', '一切', '根本', '不能', '平息', '人们', '焦虑', '的', '心情', '，', '整个', '候机', '大厅', '这时', '似乎', '已经', '变', '作', '巨型', '蜂巢', '，', '嗡嗡嘤嘤', '，', '噪声', '四起', '。', '旅客', '们', '分堆', '围住', '各', '值机', '台', '的', '机场', '工作人员', '，', '吵吵嚷嚷', '，', '无非', '是', '打听', '各自', '航班', '可能', '的', '起飞时间', '，', '追问', '补偿', '方案', '，', '等等', '。', '侯亮平', '用不着', '往前', '凑', '，', '就', '明白', '了', '一个', '意思', '：', '那片', '雷暴', '区', '只要', '在', '头顶', '罩', '着', '，', '哪个', '航班', '也', '甭想', '上天', '。']\n",
      "==================================================\n",
      "['侯亮平', '快步', '走出', '候机', '大厅', '，', '寻', '僻静处', '一个', '接', '一个', '拨打', '手机号码', '。', 'H', '省', '检察院', '检察长', '季昌明', '关机', '。', '反贪局', '局长', '陈海', '关机', '。', '当紧', '当忙', '全', '他', '妈', '失踪', '了', '。', '当然', '，', '侯亮平', '知道', '他们', '并', '没有', '失踪', '，', '而是', '在', '参加', '一个', '紧急会议', '，', '向', '该省', '分管', '政法', '工作', '的', '省委', '副', '书记', '高育良', '汇报', '丁义珍', '案件', '，', '通常', '与会者', '都', '要', '关机', '。', '但', '侯亮平', '宁愿', '相信', '他们', '是', '存心', '关机', '，', '跟', '他', '玩', '失踪', '。', '作为', '最高人民检察院', '反贪', '总局', '的', '侦查', '处处长', '，', '侯亮平', '反复', '向', 'H', '省', '的', '同行', '们', '强调', '甚至', '请求', '—', '—', '先抓', '人', '，', '后', '开会', '！', '这个', '姓', '丁', '的', '副市长', '太', '重要', '了', '，', '是', '刚', '侦破', '的', '赵德汉', '受贿案', '的', '关键', '一环', '。', '如果', '走漏风声', '让', '他', '跑', '了', '，', 'H', '省', '官场', '上', '的', '许多', '秘密', '就', '可能', '石沉', '海底', '。', '侯亮平', '对', '曾经', '的', '大学', '同学', '陈海', '尤其', '不满', '，', '他', '特地', '嘱咐', '陈海', '别', '汇报', '，', '先', '把', '丁义珍', '控制', '起来', '再说', '，', '可', '陈海', '胆小', '，', '支吾', '几句', '到底', '还是', '汇报', '了', '。', '侯亮平', '正', '因为', '害怕', '夜长梦多', '，', '抓捕', '赵德汉', '之后', '才', '在', '第一', '时间', '赶', '夜间', '航班', '飞赴', 'H', '省', '，', '不料', '偏', '又', '陷入', '了', '雷暴', '区', '。']\n"
     ]
    }
   ],
   "source": [
    "# 按行数据加载(认为每行是一个文档), 并且会将每一行的前后空格去除，并且按照空格拆分成单词集合\n",
    "sentences = word2vec.LineSentence(word_file_path) \n",
    "\n",
    "k = 0\n",
    "start = 80\n",
    "end = 85\n",
    "for sentence in sentences:\n",
    "    if k >= start:\n",
    "        print(\"=\" * 50)\n",
    "        print(sentence)\n",
    "        if k >= end:\n",
    "            break\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练方式一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-11 11:44:47,977 : INFO : collecting all words and their counts\n",
      "2020-03-11 11:44:47,978 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-11 11:44:48,052 : INFO : collected 17856 word types from a corpus of 161191 raw words and 2311 sentences\n",
      "2020-03-11 11:44:48,054 : INFO : Loading a fresh vocabulary\n",
      "2020-03-11 11:44:48,091 : INFO : effective_min_count=1 retains 17856 unique words (100% of original 17856, drops 0)\n",
      "2020-03-11 11:44:48,092 : INFO : effective_min_count=1 leaves 161191 word corpus (100% of original 161191, drops 0)\n",
      "2020-03-11 11:44:48,143 : INFO : deleting the raw counts dictionary of 17856 items\n",
      "2020-03-11 11:44:48,144 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2020-03-11 11:44:48,145 : INFO : downsampling leaves estimated 120419 word corpus (74.7% of prior 161191)\n",
      "2020-03-11 11:44:48,162 : INFO : constructing a huffman tree from 17856 words\n",
      "2020-03-11 11:44:48,616 : INFO : built huffman tree with maximum node depth 17\n",
      "2020-03-11 11:44:48,652 : INFO : estimated required memory for 17856 words and 100 dimensions: 33926400 bytes\n",
      "2020-03-11 11:44:48,653 : INFO : resetting layer weights\n",
      "2020-03-11 11:44:48,845 : INFO : training model with 3 workers on 17856 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=3\n",
      "2020-03-11 11:44:49,085 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 11:44:49,088 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 11:44:49,107 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 11:44:49,109 : INFO : EPOCH - 1 : training on 161191 raw words (120452 effective words) took 0.3s, 466808 effective words/s\n",
      "2020-03-11 11:44:49,316 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 11:44:49,318 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 11:44:49,330 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 11:44:49,331 : INFO : EPOCH - 2 : training on 161191 raw words (120446 effective words) took 0.2s, 552068 effective words/s\n",
      "2020-03-11 11:44:49,528 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 11:44:49,532 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 11:44:49,539 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 11:44:49,540 : INFO : EPOCH - 3 : training on 161191 raw words (120415 effective words) took 0.2s, 588416 effective words/s\n",
      "2020-03-11 11:44:49,738 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 11:44:49,739 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 11:44:49,743 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 11:44:49,744 : INFO : EPOCH - 4 : training on 161191 raw words (120365 effective words) took 0.2s, 597282 effective words/s\n",
      "2020-03-11 11:44:49,942 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 11:44:49,950 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 11:44:49,955 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 11:44:49,955 : INFO : EPOCH - 5 : training on 161191 raw words (120361 effective words) took 0.2s, 575953 effective words/s\n",
      "2020-03-11 11:44:49,956 : INFO : training on a 805955 raw words (602039 effective words) took 1.1s, 542479 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# 按行数据加载, 最终形成的数据格式为: list(list(string))\n",
    "sentences = word2vec.LineSentence(word_file_path) \n",
    "\n",
    "# 训练Word2Vec模型\n",
    "\"\"\"\n",
    "__init__(self, sentences=None, size=100, alpha=0.025, \n",
    "        window=5, min_count=5, max_vocab_size=None, \n",
    "        sample=0.001, seed=1, workers=3, min_alpha=0.0001, \n",
    "        sg=0, hs=0, negative=5, cbow_mean=1, \n",
    "        hashfxn=<built-in function hash>, iter=5, null_word=0, \n",
    "        trim_rule=None, sorted_vocab=1, batch_words=10000, \n",
    "        compute_loss=False, callbacks=())\n",
    "sentences: 给定文档集合\n",
    "size:转换之后的特征向量的维度大小\n",
    "window：窗口大小\n",
    "min_count：如果某一个单词出现次数小于min_count,那么该单词不计算对应的词向量\n",
    "max_vocab_size：给定最多计算的词汇数目，None表示不限制。\n",
    "sg: 1(Skip-gram) 0(CBOW)， 默认为0\n",
    "hs: 1(hierarchical softmax) 0(negative)， 默认为0\n",
    "negative: 当hs为0的时候，给定负样本数目，给定为0表示不采用负采样\n",
    "iter：模型训练的迭代次数\n",
    "alpha: 梯度下降的学习率，在更新过程会进行线性的递减到min_alpha。\n",
    "\"\"\"\n",
    "model = word2vec.Word2Vec(sentences, hs = 1,min_count = 1,\n",
    "                          window = 3,size = 100, compute_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练方式二"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-11 14:41:26,681 : INFO : collecting all words and their counts\n",
      "2020-03-11 14:41:26,687 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-11 14:41:26,773 : INFO : collected 17856 word types from a corpus of 161191 raw words and 2311 sentences\n",
      "2020-03-11 14:41:26,774 : INFO : Loading a fresh vocabulary\n",
      "2020-03-11 14:41:26,832 : INFO : effective_min_count=1 retains 17856 unique words (100% of original 17856, drops 0)\n",
      "2020-03-11 14:41:26,833 : INFO : effective_min_count=1 leaves 161191 word corpus (100% of original 161191, drops 0)\n",
      "2020-03-11 14:41:26,890 : INFO : deleting the raw counts dictionary of 17856 items\n",
      "2020-03-11 14:41:26,898 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2020-03-11 14:41:26,899 : INFO : downsampling leaves estimated 120419 word corpus (74.7% of prior 161191)\n",
      "2020-03-11 14:41:26,915 : INFO : constructing a huffman tree from 17856 words\n",
      "2020-03-11 14:41:27,709 : INFO : built huffman tree with maximum node depth 17\n",
      "2020-03-11 14:41:27,746 : INFO : estimated required memory for 17856 words and 100 dimensions: 33926400 bytes\n",
      "2020-03-11 14:41:27,747 : INFO : resetting layer weights\n",
      "2020-03-11 14:41:27,944 : INFO : training model with 3 workers on 17856 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总文档数目:2311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-11 14:41:28,156 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 14:41:28,158 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 14:41:28,169 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 14:41:28,170 : INFO : EPOCH - 1 : training on 161191 raw words (120452 effective words) took 0.2s, 546913 effective words/s\n",
      "2020-03-11 14:41:28,172 : INFO : training on a 161191 raw words (120452 effective words) took 0.2s, 532718 effective words/s\n",
      "2020-03-11 14:41:28,173 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2020-03-11 14:41:28,175 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2020-03-11 14:41:28,176 : INFO : training model with 3 workers on 17856 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=3\n",
      "2020-03-11 14:41:28,385 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 14:41:28,394 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 14:41:28,401 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 14:41:28,403 : INFO : EPOCH - 1 : training on 161191 raw words (120446 effective words) took 0.2s, 538764 effective words/s\n",
      "2020-03-11 14:41:28,404 : INFO : training on a 161191 raw words (120446 effective words) took 0.2s, 530714 effective words/s\n",
      "2020-03-11 14:41:28,406 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2020-03-11 14:41:28,407 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2020-03-11 14:41:28,408 : INFO : training model with 3 workers on 17856 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=3\n",
      "2020-03-11 14:41:28,606 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 14:41:28,608 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 14:41:28,625 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 14:41:28,627 : INFO : EPOCH - 1 : training on 161191 raw words (120415 effective words) took 0.2s, 559088 effective words/s\n",
      "2020-03-11 14:41:28,627 : INFO : training on a 161191 raw words (120415 effective words) took 0.2s, 552132 effective words/s\n",
      "2020-03-11 14:41:28,629 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2020-03-11 14:41:28,630 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2020-03-11 14:41:28,631 : INFO : training model with 3 workers on 17856 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=3\n",
      "2020-03-11 14:41:28,838 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 14:41:28,841 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 14:41:28,856 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 14:41:28,857 : INFO : EPOCH - 1 : training on 161191 raw words (120365 effective words) took 0.2s, 544413 effective words/s\n",
      "2020-03-11 14:41:28,859 : INFO : training on a 161191 raw words (120365 effective words) took 0.2s, 533206 effective words/s\n",
      "2020-03-11 14:41:28,860 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2020-03-11 14:41:28,862 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2020-03-11 14:41:28,864 : INFO : training model with 3 workers on 17856 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=3\n",
      "2020-03-11 14:41:29,085 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 14:41:29,087 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 14:41:29,101 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 14:41:29,102 : INFO : EPOCH - 1 : training on 161191 raw words (120361 effective words) took 0.2s, 512463 effective words/s\n",
      "2020-03-11 14:41:29,104 : INFO : training on a 161191 raw words (120361 effective words) took 0.2s, 505659 effective words/s\n",
      "2020-03-11 14:41:29,105 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2020-03-11 14:41:29,106 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2020-03-11 14:41:29,108 : INFO : training model with 3 workers on 17856 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=3\n",
      "2020-03-11 14:41:29,320 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 14:41:29,327 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 14:41:29,342 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 14:41:29,343 : INFO : EPOCH - 1 : training on 161191 raw words (120371 effective words) took 0.2s, 520664 effective words/s\n",
      "2020-03-11 14:41:29,346 : INFO : training on a 161191 raw words (120371 effective words) took 0.2s, 507722 effective words/s\n",
      "2020-03-11 14:41:29,348 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2020-03-11 14:41:29,350 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2020-03-11 14:41:29,352 : INFO : training model with 3 workers on 17856 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=3\n",
      "2020-03-11 14:41:29,592 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 14:41:29,593 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 14:41:29,603 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 14:41:29,604 : INFO : EPOCH - 1 : training on 161191 raw words (120482 effective words) took 0.2s, 488430 effective words/s\n",
      "2020-03-11 14:41:29,604 : INFO : training on a 161191 raw words (120482 effective words) took 0.3s, 480091 effective words/s\n",
      "2020-03-11 14:41:29,606 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2020-03-11 14:41:29,607 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2020-03-11 14:41:29,609 : INFO : training model with 3 workers on 17856 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=3\n",
      "2020-03-11 14:41:29,832 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 14:41:29,835 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 14:41:29,848 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 14:41:29,849 : INFO : EPOCH - 1 : training on 161191 raw words (120316 effective words) took 0.2s, 512093 effective words/s\n",
      "2020-03-11 14:41:29,850 : INFO : training on a 161191 raw words (120316 effective words) took 0.2s, 503674 effective words/s\n",
      "2020-03-11 14:41:29,851 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2020-03-11 14:41:29,851 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2020-03-11 14:41:29,853 : INFO : training model with 3 workers on 17856 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=3\n",
      "2020-03-11 14:41:30,076 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 14:41:30,078 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 14:41:30,082 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 14:41:30,083 : INFO : EPOCH - 1 : training on 161191 raw words (120512 effective words) took 0.2s, 530066 effective words/s\n",
      "2020-03-11 14:41:30,084 : INFO : training on a 161191 raw words (120512 effective words) took 0.2s, 523413 effective words/s\n",
      "2020-03-11 14:41:30,085 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2020-03-11 14:41:30,086 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2020-03-11 14:41:30,087 : INFO : training model with 3 workers on 17856 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=3\n",
      "2020-03-11 14:41:30,303 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 14:41:30,309 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 14:41:30,315 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 14:41:30,316 : INFO : EPOCH - 1 : training on 161191 raw words (120447 effective words) took 0.2s, 534969 effective words/s\n",
      "2020-03-11 14:41:30,318 : INFO : training on a 161191 raw words (120447 effective words) took 0.2s, 525479 effective words/s\n",
      "2020-03-11 14:41:30,319 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# 每行数据加载\n",
    "sentences = word2vec.LineSentence(word_file_path) \n",
    "\n",
    "# 创建Word2Vec模型\n",
    "\"\"\"\n",
    "__init__(self, sentences=None, size=100, alpha=0.025, \n",
    "        window=5, min_count=5, max_vocab_size=None, \n",
    "        sample=0.001, seed=1, workers=3, min_alpha=0.0001, \n",
    "        sg=0, hs=0, negative=5, cbow_mean=1, \n",
    "        hashfxn=<built-in function hash>, iter=5, null_word=0, \n",
    "        trim_rule=None, sorted_vocab=1, batch_words=10000, \n",
    "        compute_loss=False, callbacks=())\n",
    "sg: 1(Skip-gram) 0(CBOW)\n",
    "hs: 1(hierarchical softmax) 0(negative)\n",
    "negative: 当hs为0的时候，给定负样本数目，给定为0表示不采用负采样\n",
    "\"\"\"\n",
    "model = word2vec.Word2Vec(hs = 1,min_count = 1,window = 3,size = 100)\n",
    "\n",
    "# 构建词典(单词和id之间的映射关系)\n",
    "model.build_vocab(sentences)\n",
    "\n",
    "# 模型训练\n",
    "print(\"总文档数目:{}\".format(model.corpus_count))\n",
    "\n",
    "# 可以一次性迭代训练10次(10epoch)\n",
    "# model.train(sentences, total_examples=model.corpus_count, epochs=10)\n",
    "\n",
    "# 也可以每次训练1次(1个epoch)，但是每次训练后都可以进行参数修改\n",
    "for epoch in range(10):\n",
    "    model.train(sentences, total_examples=model.corpus_count, epochs=1)\n",
    "    # 可以进行参数修改\n",
    "    model.alpha -= 0.002\n",
    "    model.min_alpha = 0.001 * model.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、Word2Vec应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 获取相似度/相关性最高的K个演员\n",
    "从训练数据中获取最相似的topn个单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-11 14:41:47,973 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('田国富', 0.9411921501159668),\n",
       " ('高育良', 0.9379141330718994),\n",
       " ('季昌明', 0.9015998840332031),\n",
       " ('这位', 0.8981113433837891),\n",
       " ('祁同伟', 0.896399974822998),\n",
       " ('李达康', 0.8698292970657349),\n",
       " ('易学习', 0.8689755201339722),\n",
       " ('吴彩霞', 0.8689247369766235),\n",
       " ('做个', 0.8619071245193481),\n",
       " ('陈岩石', 0.8618505001068115)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取和\"沙瑞金\"这个单词向量最相似的topn个单词，以及这些单词的夹角余弦相似度\n",
    "model.wv.similar_by_word('沙瑞金', topn =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高育良 季昌明 田国富 李达康 侯亮平 陆亦可 易学习 陈岩石 吴慧芬 赵东来 肖钢玉 祁同伟 钟小艾 孙书记 梁璐 郑西坡 赵德汉 季检 欧阳菁 赵瑞龙 陈清泉 赵立春 吴春林 沙 孙连城 敬畏 刘总 李 达康 谢谢您\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "\n",
    "jieba.add_word('沙瑞金',10,'nr')\n",
    "jieba.add_word('田国富',10,'nr')\n",
    "jieba.add_word('高育良',10,'nr')\n",
    "jieba.add_word('侯亮平',10,'nr')\n",
    "jieba.add_word('钟小艾', 10,'nr')\n",
    "jieba.add_word('陈岩石', 10,'nr')\n",
    "jieba.add_word('欧阳菁', 10,'nr')\n",
    "jieba.add_word('易学习', 10,'nr')\n",
    "jieba.add_word('王大路', 10,'nr')\n",
    "jieba.add_word('蔡成功', 10,'nr')\n",
    "jieba.add_word('孙连城', 10,'nr')\n",
    "jieba.add_word('季昌明', 10,'nr')\n",
    "jieba.add_word('丁义珍', 10,'nr')\n",
    "jieba.add_word('郑西坡', 10,'nr')\n",
    "jieba.add_word('赵东来', 10,'nr')\n",
    "jieba.add_word('高小琴', 10,'nr')\n",
    "jieba.add_word('赵瑞龙', 10,'nr')\n",
    "jieba.add_word('林华华', 10,'nr')\n",
    "jieba.add_word('陆亦可', 10,'nr')\n",
    "jieba.add_word('刘新建', 10,'nr')\n",
    "jieba.add_word('刘庆祝', 10,'nr')\n",
    "jieba.add_word('京州市', 10,'nr')\n",
    "jieba.add_word('副市长', 10,'nr')\n",
    "jieba.add_word('赵德汉',10,'nr')\n",
    "\n",
    "\n",
    "tmp01 = model.wv.similar_by_word('沙瑞金', topn=100)\n",
    "tmp01 = ''.join(map(lambda t:t[0], tmp01))\n",
    "words = pseg.lcut(tmp01)\n",
    "result = ' '.join([word for word,flag in words if flag == 'nr'])\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高育良 0.8608838319778442\n",
      "季昌明 0.8341003656387329\n",
      "田国富 0.830956757068634\n",
      "李达康 0.8165519833564758\n",
      "侯亮平 0.7882125377655029\n"
     ]
    }
   ],
   "source": [
    "# 夹角余弦相似度\n",
    "req_count = 5\n",
    "for key in model.wv.similar_by_word('沙瑞金', topn =100):\n",
    "    if len(key[0])==3:\n",
    "        req_count -= 1\n",
    "        print(key[0], key[1])\n",
    "        if req_count == 0:\n",
    "            break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 获取单词之间的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8608839\n"
     ]
    }
   ],
   "source": [
    "# 获取单词向量的夹角余弦相似度/相关性\n",
    "print(model.wv.similarity('沙瑞金', '高育良'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 获取单词的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "[ 0.25345016 -0.0069958  -0.28571624 -0.1290999  -0.16687526 -0.13535061\n",
      " -0.19934966 -0.10971514  0.18869436 -0.33045304  0.11058953  0.15518627\n",
      " -0.27602708  0.02290593  0.13005173 -0.05552704  0.31312478 -0.15938525\n",
      " -0.02327144 -0.04423758  0.07495979 -0.03580485  0.28716636 -0.14107184\n",
      "  0.09984712 -0.13862452 -0.15097493 -0.05058463  0.12470543 -0.08889644\n",
      " -0.04234942 -0.31579977 -0.03215234  0.12022522 -0.04640454 -0.05165878\n",
      " -0.13708968  0.02664147  0.0713142  -0.55306405 -0.36513776  0.06065075\n",
      "  0.07880416  0.27386206  0.23345795 -0.14853674  0.14477734  0.13385785\n",
      " -0.4448452   0.04699555 -0.0059435  -0.01429358 -0.21083947  0.06818633\n",
      " -0.4429156   0.29825822 -0.27673772 -0.10676947 -0.04256191  0.04803126\n",
      "  0.07912153 -0.00751675  0.10841286  0.04414946  0.10697284 -0.08935854\n",
      " -0.06691136  0.22935225 -0.00337701 -0.42719752  0.02492289 -0.11838258\n",
      " -0.12504445 -0.07779804 -0.2023888  -0.4704439   0.10455754 -0.06818312\n",
      " -0.31577206  0.10807444 -0.03278089 -0.5052584  -0.19051062  0.22609417\n",
      " -0.34271687 -0.10076331 -0.07964238 -0.09788565  0.01872945  0.34026963\n",
      " -0.01497107  0.02652641 -0.1201051  -0.01383507 -0.30182028 -0.24549182\n",
      " -0.05890299 -0.39740247 -0.01472266  0.20587225]\n"
     ]
    }
   ],
   "source": [
    "v1 = model.wv[\"提拔\"]\n",
    "print(v1.shape)\n",
    "print(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "[ 0.25345016 -0.0069958  -0.28571624 -0.1290999  -0.16687526 -0.13535061\n",
      " -0.19934966 -0.10971514  0.18869436 -0.33045304  0.11058953  0.15518627\n",
      " -0.27602708  0.02290593  0.13005173 -0.05552704  0.31312478 -0.15938525\n",
      " -0.02327144 -0.04423758  0.07495979 -0.03580485  0.28716636 -0.14107184\n",
      "  0.09984712 -0.13862452 -0.15097493 -0.05058463  0.12470543 -0.08889644\n",
      " -0.04234942 -0.31579977 -0.03215234  0.12022522 -0.04640454 -0.05165878\n",
      " -0.13708968  0.02664147  0.0713142  -0.55306405 -0.36513776  0.06065075\n",
      "  0.07880416  0.27386206  0.23345795 -0.14853674  0.14477734  0.13385785\n",
      " -0.4448452   0.04699555 -0.0059435  -0.01429358 -0.21083947  0.06818633\n",
      " -0.4429156   0.29825822 -0.27673772 -0.10676947 -0.04256191  0.04803126\n",
      "  0.07912153 -0.00751675  0.10841286  0.04414946  0.10697284 -0.08935854\n",
      " -0.06691136  0.22935225 -0.00337701 -0.42719752  0.02492289 -0.11838258\n",
      " -0.12504445 -0.07779804 -0.2023888  -0.4704439   0.10455754 -0.06818312\n",
      " -0.31577206  0.10807444 -0.03278089 -0.5052584  -0.19051062  0.22609417\n",
      " -0.34271687 -0.10076331 -0.07964238 -0.09788565  0.01872945  0.34026963\n",
      " -0.01497107  0.02652641 -0.1201051  -0.01383507 -0.30182028 -0.24549182\n",
      " -0.05890299 -0.39740247 -0.01472266  0.20587225]\n"
     ]
    }
   ],
   "source": [
    "v1 = model.wv.get_vector(\"提拔\")\n",
    "print(v1.shape)\n",
    "print(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 异常：不存在\"小明\"这个单词\n",
    "word = \"小明\"\n",
    "if word in model.wv:\n",
    "    print(model.wv[word])\n",
    "\"小明\" not in model.wv\n",
    "# model.wv.get_vector(\"小明\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、模型持久化&模型恢复加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方式一：\n",
    "直接使用save API进行模型持久化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-11 11:57:12,516 : INFO : saving Word2Vec object under ./datas/gensim_word2vec1.w2v, separately None\n",
      "2020-03-11 11:57:12,518 : INFO : not storing attribute vectors_norm\n",
      "2020-03-11 11:57:12,519 : INFO : not storing attribute cum_table\n",
      "2020-03-11 11:57:13,076 : INFO : saved ./datas/gensim_word2vec1.w2v\n"
     ]
    }
   ],
   "source": [
    "model.save(model_file_path1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-11 11:57:47,280 : INFO : loading Word2Vec object from ./datas/gensim_word2vec1.w2v\n",
      "2020-03-11 11:57:47,537 : INFO : loading wv recursively from ./datas/gensim_word2vec1.w2v.wv.* with mmap=None\n",
      "2020-03-11 11:57:47,538 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-03-11 11:57:47,539 : INFO : loading vocabulary recursively from ./datas/gensim_word2vec1.w2v.vocabulary.* with mmap=None\n",
      "2020-03-11 11:57:47,540 : INFO : loading trainables recursively from ./datas/gensim_word2vec1.w2v.trainables.* with mmap=None\n",
      "2020-03-11 11:57:47,541 : INFO : setting ignored attribute cum_table to None\n",
      "2020-03-11 11:57:47,543 : INFO : loaded ./datas/gensim_word2vec1.w2v\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=17856, size=100, alpha=0.025)\n",
      "(100,)\n",
      "[ 0.25345016 -0.0069958  -0.28571624 -0.1290999  -0.16687526 -0.13535061\n",
      " -0.19934966 -0.10971514  0.18869436 -0.33045304  0.11058953  0.15518627\n",
      " -0.27602708  0.02290593  0.13005173 -0.05552704  0.31312478 -0.15938525\n",
      " -0.02327144 -0.04423758  0.07495979 -0.03580485  0.28716636 -0.14107184\n",
      "  0.09984712 -0.13862452 -0.15097493 -0.05058463  0.12470543 -0.08889644\n",
      " -0.04234942 -0.31579977 -0.03215234  0.12022522 -0.04640454 -0.05165878\n",
      " -0.13708968  0.02664147  0.0713142  -0.55306405 -0.36513776  0.06065075\n",
      "  0.07880416  0.27386206  0.23345795 -0.14853674  0.14477734  0.13385785\n",
      " -0.4448452   0.04699555 -0.0059435  -0.01429358 -0.21083947  0.06818633\n",
      " -0.4429156   0.29825822 -0.27673772 -0.10676947 -0.04256191  0.04803126\n",
      "  0.07912153 -0.00751675  0.10841286  0.04414946  0.10697284 -0.08935854\n",
      " -0.06691136  0.22935225 -0.00337701 -0.42719752  0.02492289 -0.11838258\n",
      " -0.12504445 -0.07779804 -0.2023888  -0.4704439   0.10455754 -0.06818312\n",
      " -0.31577206  0.10807444 -0.03278089 -0.5052584  -0.19051062  0.22609417\n",
      " -0.34271687 -0.10076331 -0.07964238 -0.09788565  0.01872945  0.34026963\n",
      " -0.01497107  0.02652641 -0.1201051  -0.01383507 -0.30182028 -0.24549182\n",
      " -0.05890299 -0.39740247 -0.01472266  0.20587225]\n"
     ]
    }
   ],
   "source": [
    "# 直接基于路径加载\n",
    "model2 = word2vec.Word2Vec.load(model_file_path1)\n",
    "print(model2)\n",
    "\n",
    "v1 = model2.wv.get_vector(\"提拔\")\n",
    "print(v1.shape)\n",
    "print(v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方式二：\n",
    "保存为二进制词向量或者文本向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-11 12:01:41,069 : INFO : storing 17856x100 projection weights into ./datas/gensim_word2vec2.bin\n"
     ]
    }
   ],
   "source": [
    "# 将数据保存为二进制的格式\n",
    "model.wv.save_word2vec_format(model_file_path2,binary=True)\n",
    "\n",
    "# 将数据保存为txt文本格式\n",
    "# model.wv.save_word2vec_format(model_file_path2,binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-11 12:02:02,389 : INFO : loading projection weights from ./datas/gensim_word2vec2.bin\n",
      "2020-03-11 12:02:02,861 : INFO : loaded (17856, 100) matrix from ./datas/gensim_word2vec2.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x000001262FD98240>\n",
      "(100,)\n",
      "[ 0.25345016 -0.0069958  -0.28571624 -0.1290999  -0.16687526 -0.13535061\n",
      " -0.19934966 -0.10971514  0.18869436 -0.33045304  0.11058953  0.15518627\n",
      " -0.27602708  0.02290593  0.13005173 -0.05552704  0.31312478 -0.15938525\n",
      " -0.02327144 -0.04423758  0.07495979 -0.03580485  0.28716636 -0.14107184\n",
      "  0.09984712 -0.13862452 -0.15097493 -0.05058463  0.12470543 -0.08889644\n",
      " -0.04234942 -0.31579977 -0.03215234  0.12022522 -0.04640454 -0.05165878\n",
      " -0.13708968  0.02664147  0.0713142  -0.55306405 -0.36513776  0.06065075\n",
      "  0.07880416  0.27386206  0.23345795 -0.14853674  0.14477734  0.13385785\n",
      " -0.4448452   0.04699555 -0.0059435  -0.01429358 -0.21083947  0.06818633\n",
      " -0.4429156   0.29825822 -0.27673772 -0.10676947 -0.04256191  0.04803126\n",
      "  0.07912153 -0.00751675  0.10841286  0.04414946  0.10697284 -0.08935854\n",
      " -0.06691136  0.22935225 -0.00337701 -0.42719752  0.02492289 -0.11838258\n",
      " -0.12504445 -0.07779804 -0.2023888  -0.4704439   0.10455754 -0.06818312\n",
      " -0.31577206  0.10807444 -0.03278089 -0.5052584  -0.19051062  0.22609417\n",
      " -0.34271687 -0.10076331 -0.07964238 -0.09788565  0.01872945  0.34026963\n",
      " -0.01497107  0.02652641 -0.1201051  -0.01383507 -0.30182028 -0.24549182\n",
      " -0.05890299 -0.39740247 -0.01472266  0.20587225]\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "# 加载二进制格式保存的模型\n",
    "model2 = gensim.models.KeyedVectors.load_word2vec_format(model_file_path2,binary=True)\n",
    "\n",
    "# 加载txt文本格式模型数据\n",
    "# model2 = gensim.models.KeyedVectors.load_word2vec_format(model_file_path2,binary=False)\n",
    "print(model2)\n",
    "\n",
    "# 应用模型\n",
    "v1 = model2.get_vector(\"提拔\")\n",
    "print(v1.shape)\n",
    "print(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-11 12:04:32,800 : INFO : loading projection weights from ./datas/vectors.bin\n",
      "2020-03-11 12:04:32,992 : INFO : loaded (7942, 128) matrix from ./datas/vectors.bin\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:16: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "2020-03-11 12:04:32,997 : INFO : storing 7942x128 projection weights into ./datas/vectors.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x000001265B3D61D0>\n",
      "【酒店】:\n",
      "[ 0.07507085 -0.06233635 -0.08300601 -0.09476656  0.19525695  0.08683676\n",
      "  0.267884    0.03470918  0.14521684  0.00294149 -0.02735998 -0.09170757\n",
      "  0.00139181  0.2421832  -0.09511401  0.0341509   0.0283838   0.32073992\n",
      "  0.172025    0.20465788  0.05929533  0.07462119 -0.23834492  0.0421031\n",
      " -0.0448295  -0.02866336  0.05001667 -0.1257836   0.22431172 -0.0807058\n",
      " -0.12493779 -0.05265829  0.13126895  0.15061386 -0.19615541 -0.09053446\n",
      " -0.05627611 -0.23135136 -0.01231913 -0.23680945 -0.04299964  0.3667591\n",
      " -0.06821534 -0.29599202  0.34265348 -0.04311483 -0.21866152 -0.2495054\n",
      " -0.43372962  0.0463162   0.11516414  0.07433167  0.09803177 -0.06165684\n",
      " -0.1319202   0.04795204  0.2651979   0.09628078 -0.16381025  0.12577897\n",
      "  0.11097272 -0.15810862 -0.07860032  0.08296187  0.12618895  0.28030068\n",
      " -0.00506122  0.05640052  0.11889897 -0.11027029 -0.14830197 -0.00842615\n",
      "  0.01188814  0.19347009  0.00282715 -0.3220406   0.22365303 -0.23093392\n",
      " -0.41848904 -0.19116873 -0.12905827  0.29085568 -0.33723694 -0.21085052\n",
      "  0.04453738 -0.07304095 -0.0262253   0.19612503  0.33479366  0.14669509\n",
      "  0.12059527 -0.00447296 -0.05844707  0.08822253  0.0886934   0.03672414\n",
      "  0.03962974  0.18999839  0.18470356  0.01651068  0.06237809  0.31986848\n",
      "  0.07099832  0.08987687  0.03214902 -0.0823281  -0.18802191  0.02356046\n",
      " -0.05299398 -0.01493336  0.14630602 -0.06124404  0.08387742 -0.11301316\n",
      "  0.26206592 -0.05086057 -0.15222454  0.22112454 -0.11467611  0.13941425\n",
      "  0.08904102  0.13969192  0.1377303   0.15661885  0.14377183  0.05035875\n",
      "  0.11127736  0.24495345]\n"
     ]
    }
   ],
   "source": [
    "# 加载其它模型(一般用于加载别人训练好的Word2Vec模型参数<开源>)\n",
    "other_model_file_path = './datas/vectors.bin'\n",
    "other_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    other_model_file_path,binary=True)\n",
    "print(other_model)\n",
    "\n",
    "# 应用模型\n",
    "word = '提拔'\n",
    "word = '酒店'\n",
    "if word in other_model:\n",
    "    print(\"【{}】:\\n{}\".format(word, other_model[word]))\n",
    "else:\n",
    "    print(\"【{}】不存在！！！\".format(word))\n",
    "\n",
    "# 将模型保存为文本形式\n",
    "other_model.wv.save_word2vec_format('./datas/vectors.txt',binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 方式三：\n",
    "直接使用NumPy API保存词向量信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17856, 100) (17856, 100) (17856, 2)\n"
     ]
    }
   ],
   "source": [
    "# 获取词向量\n",
    "norm_word_embeddings = model.wv.vectors_norm # 经过L2转换后的词向量映射矩阵\n",
    "word_embeddings = model.wv.vectors # 原始词向量映射矩阵\n",
    "# 获取词典(词典到idx的映射)\n",
    "vocab_2_index = list(map(lambda k: (k, model.wv.vocab[k].index), model.wv.vocab))\n",
    "print(np.shape(norm_word_embeddings), np.shape(word_embeddings), np.shape(vocab_2_index))\n",
    "# 数据保存\n",
    "np.save(model_file_path3.format(\"norm_embedding\"), norm_word_embeddings)\n",
    "np.save(model_file_path3.format(\"embedding\"), word_embeddings)\n",
    "np.save(model_file_path3.format(\"vocab_2_index\"), vocab_2_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "[ 0.25345016 -0.0069958  -0.28571624 -0.1290999  -0.16687526 -0.13535061\n",
      " -0.19934966 -0.10971514  0.18869436 -0.33045304  0.11058953  0.15518627\n",
      " -0.27602708  0.02290593  0.13005173 -0.05552704  0.31312478 -0.15938525\n",
      " -0.02327144 -0.04423758  0.07495979 -0.03580485  0.28716636 -0.14107184\n",
      "  0.09984712 -0.13862452 -0.15097493 -0.05058463  0.12470543 -0.08889644\n",
      " -0.04234942 -0.31579977 -0.03215234  0.12022522 -0.04640454 -0.05165878\n",
      " -0.13708968  0.02664147  0.0713142  -0.55306405 -0.36513776  0.06065075\n",
      "  0.07880416  0.27386206  0.23345795 -0.14853674  0.14477734  0.13385785\n",
      " -0.4448452   0.04699555 -0.0059435  -0.01429358 -0.21083947  0.06818633\n",
      " -0.4429156   0.29825822 -0.27673772 -0.10676947 -0.04256191  0.04803126\n",
      "  0.07912153 -0.00751675  0.10841286  0.04414946  0.10697284 -0.08935854\n",
      " -0.06691136  0.22935225 -0.00337701 -0.42719752  0.02492289 -0.11838258\n",
      " -0.12504445 -0.07779804 -0.2023888  -0.4704439   0.10455754 -0.06818312\n",
      " -0.31577206  0.10807444 -0.03278089 -0.5052584  -0.19051062  0.22609417\n",
      " -0.34271687 -0.10076331 -0.07964238 -0.09788565  0.01872945  0.34026963\n",
      " -0.01497107  0.02652641 -0.1201051  -0.01383507 -0.30182028 -0.24549182\n",
      " -0.05890299 -0.39740247 -0.01472266  0.20587225]\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "norm_word_embeddings = np.load(model_file_path3.format(\"norm_embedding\"))\n",
    "word_embeddings = np.load(model_file_path3.format(\"embedding\"))\n",
    "vocab_2_index = np.load(model_file_path3.format(\"vocab_2_index\"))\n",
    "\n",
    "# 字典转换\n",
    "vocab_2_index = dict(map(lambda t:(t[0], int(t[1])), vocab_2_index))\n",
    "\n",
    "# 获取数据\n",
    "word = \"提拔\"\n",
    "index = vocab_2_index[word] # 得到单词对应的id索引\n",
    "v1 = word_embeddings[index] # 根据id索引获取对应的向量\n",
    "print(v1.shape)\n",
    "print(v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、扩展：直接从文件中读取数据来进行模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-11 12:06:41,526 : INFO : collecting all words and their counts\n",
      "2020-03-11 12:06:41,528 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-11 12:06:41,663 : INFO : collected 17800 word types from a corpus of 131991 raw words and 2422 sentences\n",
      "2020-03-11 12:06:41,664 : INFO : Loading a fresh vocabulary\n",
      "2020-03-11 12:06:41,701 : INFO : effective_min_count=1 retains 17800 unique words (100% of original 17800, drops 0)\n",
      "2020-03-11 12:06:41,702 : INFO : effective_min_count=1 leaves 131991 word corpus (100% of original 131991, drops 0)\n",
      "2020-03-11 12:06:41,760 : INFO : deleting the raw counts dictionary of 17800 items\n",
      "2020-03-11 12:06:41,762 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2020-03-11 12:06:41,762 : INFO : downsampling leaves estimated 111227 word corpus (84.3% of prior 131991)\n",
      "2020-03-11 12:06:41,779 : INFO : constructing a huffman tree from 17800 words\n",
      "2020-03-11 12:06:42,239 : INFO : built huffman tree with maximum node depth 17\n",
      "2020-03-11 12:06:42,270 : INFO : estimated required memory for 17800 words and 100 dimensions: 33820000 bytes\n",
      "2020-03-11 12:06:42,272 : INFO : resetting layer weights\n",
      "2020-03-11 12:06:42,467 : INFO : training model with 3 workers on 17800 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=3\n",
      "2020-03-11 12:06:42,698 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:42,702 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:42,717 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:42,718 : INFO : EPOCH - 1 : training on 131991 raw words (111269 effective words) took 0.2s, 451781 effective words/s\n",
      "2020-03-11 12:06:42,981 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:42,983 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:42,998 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:42,999 : INFO : EPOCH - 2 : training on 131991 raw words (111161 effective words) took 0.3s, 397985 effective words/s\n",
      "2020-03-11 12:06:43,246 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:43,248 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:43,260 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:43,261 : INFO : EPOCH - 3 : training on 131991 raw words (111362 effective words) took 0.3s, 431660 effective words/s\n",
      "2020-03-11 12:06:43,498 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:43,504 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:43,508 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:43,509 : INFO : EPOCH - 4 : training on 131991 raw words (111336 effective words) took 0.2s, 453444 effective words/s\n",
      "2020-03-11 12:06:43,731 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:43,735 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:43,748 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:43,748 : INFO : EPOCH - 5 : training on 131991 raw words (111197 effective words) took 0.2s, 470046 effective words/s\n",
      "2020-03-11 12:06:43,984 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:43,987 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:43,999 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:44,000 : INFO : EPOCH - 6 : training on 131991 raw words (111313 effective words) took 0.2s, 446928 effective words/s\n",
      "2020-03-11 12:06:44,227 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:44,235 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:44,242 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:44,243 : INFO : EPOCH - 7 : training on 131991 raw words (111256 effective words) took 0.2s, 464040 effective words/s\n",
      "2020-03-11 12:06:44,494 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:44,497 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:44,512 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:44,513 : INFO : EPOCH - 8 : training on 131991 raw words (111162 effective words) took 0.3s, 415418 effective words/s\n",
      "2020-03-11 12:06:44,770 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:44,773 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:44,785 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:44,787 : INFO : EPOCH - 9 : training on 131991 raw words (111275 effective words) took 0.3s, 415236 effective words/s\n",
      "2020-03-11 12:06:45,035 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:45,039 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:45,053 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:45,054 : INFO : EPOCH - 10 : training on 131991 raw words (111306 effective words) took 0.3s, 419880 effective words/s\n",
      "2020-03-11 12:06:45,322 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:45,326 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:45,340 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:45,341 : INFO : EPOCH - 11 : training on 131991 raw words (111216 effective words) took 0.3s, 392709 effective words/s\n",
      "2020-03-11 12:06:45,575 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:45,577 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:45,590 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:45,591 : INFO : EPOCH - 12 : training on 131991 raw words (111017 effective words) took 0.2s, 447311 effective words/s\n",
      "2020-03-11 12:06:45,813 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:45,824 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:45,832 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:45,834 : INFO : EPOCH - 13 : training on 131991 raw words (111436 effective words) took 0.2s, 465555 effective words/s\n",
      "2020-03-11 12:06:46,058 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:46,067 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:46,081 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:46,083 : INFO : EPOCH - 14 : training on 131991 raw words (111191 effective words) took 0.2s, 450574 effective words/s\n",
      "2020-03-11 12:06:46,311 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:46,313 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:46,321 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:46,322 : INFO : EPOCH - 15 : training on 131991 raw words (111402 effective words) took 0.2s, 474458 effective words/s\n",
      "2020-03-11 12:06:46,544 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:46,553 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:46,563 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:46,564 : INFO : EPOCH - 16 : training on 131991 raw words (111210 effective words) took 0.2s, 468685 effective words/s\n",
      "2020-03-11 12:06:46,808 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:46,814 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:46,830 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:46,831 : INFO : EPOCH - 17 : training on 131991 raw words (111196 effective words) took 0.3s, 424008 effective words/s\n",
      "2020-03-11 12:06:47,058 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:47,063 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:47,079 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:47,081 : INFO : EPOCH - 18 : training on 131991 raw words (111300 effective words) took 0.2s, 451636 effective words/s\n",
      "2020-03-11 12:06:47,315 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:47,319 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:47,337 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:47,338 : INFO : EPOCH - 19 : training on 131991 raw words (111229 effective words) took 0.3s, 438080 effective words/s\n",
      "2020-03-11 12:06:47,576 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:47,579 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:47,596 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:47,597 : INFO : EPOCH - 20 : training on 131991 raw words (111311 effective words) took 0.3s, 431598 effective words/s\n",
      "2020-03-11 12:06:47,829 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:47,831 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:47,845 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:47,847 : INFO : EPOCH - 21 : training on 131991 raw words (111208 effective words) took 0.2s, 451294 effective words/s\n",
      "2020-03-11 12:06:48,083 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:48,085 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:48,101 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:48,102 : INFO : EPOCH - 22 : training on 131991 raw words (111342 effective words) took 0.3s, 441273 effective words/s\n",
      "2020-03-11 12:06:48,342 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:48,344 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:48,350 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:48,351 : INFO : EPOCH - 23 : training on 131991 raw words (111072 effective words) took 0.2s, 450955 effective words/s\n",
      "2020-03-11 12:06:48,580 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:48,589 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:48,597 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:48,598 : INFO : EPOCH - 24 : training on 131991 raw words (111270 effective words) took 0.2s, 456416 effective words/s\n",
      "2020-03-11 12:06:48,836 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:48,838 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:48,857 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:48,858 : INFO : EPOCH - 25 : training on 131991 raw words (111230 effective words) took 0.3s, 431892 effective words/s\n",
      "2020-03-11 12:06:49,122 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:49,125 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:49,147 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:49,148 : INFO : EPOCH - 26 : training on 131991 raw words (111093 effective words) took 0.3s, 388099 effective words/s\n",
      "2020-03-11 12:06:49,453 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:49,459 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:49,481 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:49,482 : INFO : EPOCH - 27 : training on 131991 raw words (111253 effective words) took 0.3s, 337178 effective words/s\n",
      "2020-03-11 12:06:49,774 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:49,778 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:49,795 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:49,796 : INFO : EPOCH - 28 : training on 131991 raw words (111208 effective words) took 0.3s, 359370 effective words/s\n",
      "2020-03-11 12:06:50,099 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:50,107 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:50,121 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:50,122 : INFO : EPOCH - 29 : training on 131991 raw words (111213 effective words) took 0.3s, 345299 effective words/s\n",
      "2020-03-11 12:06:50,413 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:50,418 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:50,437 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:50,438 : INFO : EPOCH - 30 : training on 131991 raw words (111248 effective words) took 0.3s, 355092 effective words/s\n",
      "2020-03-11 12:06:50,779 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:50,781 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:50,802 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:50,804 : INFO : EPOCH - 31 : training on 131991 raw words (111226 effective words) took 0.4s, 307933 effective words/s\n",
      "2020-03-11 12:06:51,100 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:51,102 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:51,126 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:51,127 : INFO : EPOCH - 32 : training on 131991 raw words (111333 effective words) took 0.3s, 347895 effective words/s\n",
      "2020-03-11 12:06:51,422 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:51,427 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:51,445 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:51,446 : INFO : EPOCH - 33 : training on 131991 raw words (111305 effective words) took 0.3s, 352562 effective words/s\n",
      "2020-03-11 12:06:51,737 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:51,746 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:51,761 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:51,762 : INFO : EPOCH - 34 : training on 131991 raw words (111265 effective words) took 0.3s, 356866 effective words/s\n",
      "2020-03-11 12:06:52,054 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:52,062 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:52,077 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:52,079 : INFO : EPOCH - 35 : training on 131991 raw words (111206 effective words) took 0.3s, 355662 effective words/s\n",
      "2020-03-11 12:06:52,376 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:52,381 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:52,405 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:52,406 : INFO : EPOCH - 36 : training on 131991 raw words (111198 effective words) took 0.3s, 344149 effective words/s\n",
      "2020-03-11 12:06:52,698 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:52,703 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:52,719 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:52,720 : INFO : EPOCH - 37 : training on 131991 raw words (111146 effective words) took 0.3s, 358491 effective words/s\n",
      "2020-03-11 12:06:52,990 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:52,997 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:53,011 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:53,012 : INFO : EPOCH - 38 : training on 131991 raw words (111219 effective words) took 0.3s, 386110 effective words/s\n",
      "2020-03-11 12:06:53,302 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:53,317 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:53,330 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:53,331 : INFO : EPOCH - 39 : training on 131991 raw words (111329 effective words) took 0.3s, 353941 effective words/s\n",
      "2020-03-11 12:06:53,612 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:53,617 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:53,636 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:53,638 : INFO : EPOCH - 40 : training on 131991 raw words (111131 effective words) took 0.3s, 366641 effective words/s\n",
      "2020-03-11 12:06:53,923 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:53,924 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:53,950 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:53,952 : INFO : EPOCH - 41 : training on 131991 raw words (111319 effective words) took 0.3s, 359883 effective words/s\n",
      "2020-03-11 12:06:54,241 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:54,243 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:54,263 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:54,264 : INFO : EPOCH - 42 : training on 131991 raw words (111258 effective words) took 0.3s, 361656 effective words/s\n",
      "2020-03-11 12:06:54,555 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:54,557 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:54,567 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:54,568 : INFO : EPOCH - 43 : training on 131991 raw words (111203 effective words) took 0.3s, 370059 effective words/s\n",
      "2020-03-11 12:06:54,846 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:54,850 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:54,872 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:54,875 : INFO : EPOCH - 44 : training on 131991 raw words (111338 effective words) took 0.3s, 366912 effective words/s\n",
      "2020-03-11 12:06:55,167 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:55,169 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:55,188 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:55,189 : INFO : EPOCH - 45 : training on 131991 raw words (111130 effective words) took 0.3s, 359290 effective words/s\n",
      "2020-03-11 12:06:55,472 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:55,490 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:55,504 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:55,505 : INFO : EPOCH - 46 : training on 131991 raw words (111338 effective words) took 0.3s, 355518 effective words/s\n",
      "2020-03-11 12:06:55,789 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:55,793 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:55,811 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:55,813 : INFO : EPOCH - 47 : training on 131991 raw words (111288 effective words) took 0.3s, 365903 effective words/s\n",
      "2020-03-11 12:06:56,106 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:56,115 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:56,129 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:56,130 : INFO : EPOCH - 48 : training on 131991 raw words (111244 effective words) took 0.3s, 354976 effective words/s\n",
      "2020-03-11 12:06:56,413 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:56,422 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:56,438 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:56,440 : INFO : EPOCH - 49 : training on 131991 raw words (111304 effective words) took 0.3s, 362893 effective words/s\n",
      "2020-03-11 12:06:56,724 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-11 12:06:56,730 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-11 12:06:56,748 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-11 12:06:56,749 : INFO : EPOCH - 50 : training on 131991 raw words (111253 effective words) took 0.3s, 365140 effective words/s\n",
      "2020-03-11 12:06:56,750 : INFO : training on a 6599550 raw words (5562315 effective words) took 14.3s, 389477 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim import utils\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "\n",
    "class MyData(object):\n",
    "    def __iter__(self):\n",
    "        path = word_file_path\n",
    "        with open(path, 'r', encoding='utf-8') as reader:\n",
    "            for line in reader:\n",
    "                yield list(utils.tokenize(line))\n",
    "\n",
    "# 模型构建\n",
    "model = Word2Vec(hs = 1,min_count = 1,window = 3,size = 100, sentences=MyData(), iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-11 12:07:03,280 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "李达康 0.664504885673523\n",
      "高育良 0.6642884016036987\n",
      "季昌明 0.5894726514816284\n",
      "田国富 0.5871814489364624\n",
      "易学习 0.553726315498352\n"
     ]
    }
   ],
   "source": [
    "# 夹角余弦相似度\n",
    "req_count = 5\n",
    "for key in model.wv.similar_by_word('沙瑞金', topn =100):\n",
    "    if len(key[0])==3:\n",
    "        req_count -= 1\n",
    "        print(key[0], key[1])\n",
    "        if req_count == 0:\n",
    "            break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 六、效果可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 解决中文显示问题\n",
    "mpl.rcParams['font.sans-serif'] = [u'SimHei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  app.launch_new_instance()\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "# 获取词向量\n",
    "word_embeddings = model.wv.vectors # 原始词向量映射矩阵\n",
    "# 获取词典(词典到idx的映射)\n",
    "index_2_vocab = dict(list(map(lambda k: (model.wv.vocab[k].index, k), model.wv.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:18: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "viz_words = 1000\n",
    "tsne = TSNE() # 将高维数据映射到低维空间上，主要目的是为了可视化\n",
    "# 直接可视化前100个单词\n",
    "# embed_tsne = tsne.fit_transform(word_embeddings[:viz_words, :])\n",
    "\n",
    "# 可视化比较特殊的一些单词\n",
    "words = list(filter(lambda word: len(word) == 2, [t[1] for t in index_2_vocab.items()]))\n",
    "words = words[1500: 1500+viz_words]\n",
    "\n",
    "# words = ['侯处长','沙瑞金','田国富','高育良','侯亮平','钟小艾',\n",
    "#         '陈岩石','欧阳菁','易学习','王大路','蔡成功',\n",
    "#         '孙连城','季昌明','丁义珍','郑西坡','赵东来',\n",
    "#         '高小琴','赵瑞龙','陆亦可','刘新建',\n",
    "#         '刘庆祝','京州市','副市长','赵德汉']\n",
    "\n",
    "\n",
    "viz_words = len(words)\n",
    "indexs = [model.wv.vocab[word].index for word in words]\n",
    "embed_tsne = tsne.fit_transform(word_embeddings[indexs, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "for idx in range(viz_words):\n",
    "    word_idx = indexs[idx]\n",
    "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(index_2_vocab[word_idx], \n",
    "                 (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
